<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="分词和上下文长度分词是文本和token相互转化的过程，词汇表越大，文本映射到token的损失越少，能保留的信息越多，但同时会带来模型参数量和计算量的提升，因此词汇表大小会存在一个取舍，下文会计算对应的增量 上下文长度指的是模型最大能关注到的文本长度，上下文长度越大模型能同时关注更多的信息，当前私有模型要么通过使用私有数据微调，要么通过增加上下文长度把所有数据一次性给模型。 中文分词中文分词流程：">
<meta property="og:type" content="article">
<meta property="og:title" content="Parameter count">
<meta property="og:url" content="http://example.com/2024/04/04/Parameter-count/index.html">
<meta property="og:site_name" content="Llfeng">
<meta property="og:description" content="分词和上下文长度分词是文本和token相互转化的过程，词汇表越大，文本映射到token的损失越少，能保留的信息越多，但同时会带来模型参数量和计算量的提升，因此词汇表大小会存在一个取舍，下文会计算对应的增量 上下文长度指的是模型最大能关注到的文本长度，上下文长度越大模型能同时关注更多的信息，当前私有模型要么通过使用私有数据微调，要么通过增加上下文长度把所有数据一次性给模型。 中文分词中文分词流程：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/04/04/Parameter-count/image-20240125111241738.png">
<meta property="og:image" content="http://example.com/2024/04/04/Parameter-count/image-20240307171018254.png">
<meta property="og:image" content="http://example.com/2024/04/04/Parameter-count/v2-7276937c93e0ab2cbb74ad33f3c83a8e_1440w.webp">
<meta property="og:image" content="http://example.com/2024/04/04/Parameter-count/image-20240310093550323.png">
<meta property="og:image" content="http://example.com/2024/04/04/Parameter-count/image-20240125113050750.png">
<meta property="og:image" content="http://example.com/2024/04/04/Parameter-count/MBXY-CR-deab4f3ddb8e4e128bec48d43632f5e7.png">
<meta property="article:published_time" content="2024-04-04T10:06:56.000Z">
<meta property="article:modified_time" content="2024-04-04T10:17:33.382Z">
<meta property="article:author" content="Llfeng">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/04/04/Parameter-count/image-20240125111241738.png">

<link rel="canonical" href="http://example.com/2024/04/04/Parameter-count/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>Parameter count | Llfeng</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Llfeng</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-fw fa-download"></i>resources</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/04/Parameter-count/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/img.png">
      <meta itemprop="name" content="Llfeng">
      <meta itemprop="description" content="Welcome">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Llfeng">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Parameter count
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-04-04 18:06:56 / 修改时间：18:17:33" itemprop="dateCreated datePublished" datetime="2024-04-04T18:06:56+08:00">2024-04-04</time>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="分词和上下文长度"><a href="#分词和上下文长度" class="headerlink" title="分词和上下文长度"></a>分词和上下文长度</h2><p>分词是文本和token相互转化的过程，词汇表越大，文本映射到token的损失越少，能保留的信息越多，但同时会带来模型参数量和计算量的提升，因此词汇表大小会存在一个取舍，下文会计算对应的增量</p>
<p>上下文长度指的是模型最大能关注到的文本长度，上下文长度越大模型能同时关注更多的信息，当前私有模型要么通过使用私有数据微调，要么通过增加上下文长度把所有数据一次性给模型。</p>
<h3 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h3><p>中文分词流程：</p>
<ol>
<li>将输入的文本用正则表达式进行分词(token),</li>
<li>将每个token都转utf-8格式(token_utf8)</li>
<li>把utf-8格式的数字数组(token_utf8) 逐个转化为unicode格式的(token_byte)字符</li>
<li>把unicode格式的token_byte逐个分解成字母，然后用已有的合并表合并，直到没有可以合并的返回token_byte_bpe,</li>
<li>逐个将token_byte_bpe用训练好的编码表映到对应的编码后的tokenid(bpe_tokens)</li>
</ol>
<ul>
<li>代码对应<a target="_blank" rel="noopener" href="https://github.com/Llfeng18/Ai_Exercise_Archive/blob/main/bpe/gpt_encode.py">gpt2 code</a></li>
</ul>
<table>
<thead>
<tr>
<th>模型</th>
<th>中文token数量</th>
<th>总token数量</th>
<th>占比</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://github.com/Llfeng18/Ai_Exercise_Archive/blob/main/bpe/cl100k_base.txt">gpt4</a></td>
<td>840（去处符号762个不重复的）</td>
<td>100256</td>
<td>0.83%</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://github.com/Llfeng18/Ai_Exercise_Archive/blob/main/bpe/encoder.json">gpt2</a></td>
<td>0</td>
<td>50257</td>
<td>0</td>
</tr>
</tbody></table>
<ul>
<li>使用p50k_base和gpt2分词中文，大概1个中文两个token，cl100k_base大概是1:1，<a target="_blank" rel="noopener" href="https://tiktoken.aigc2d.com/">case</a></li>
<li>据不完全统计当前汉字大概有9w个，常见中文字3500个，token是词和词组的概念，暂不确认预期是多少，gpt4的中文词组应该也是用bpe和出来的，看着单词不是都是常用的，具体可以看<a target="_blank" rel="noopener" href="https://github.com/Llfeng18/Ai_Exercise_Archive/blob/main/bpe/gpt_encode.py">代码 gpt4_token函数</a></li>
</ul>
<table>
<thead>
<tr>
<th>文本</th>
<th>token</th>
<th>token对应编码</th>
</tr>
</thead>
<tbody><tr>
<td>绍</td>
<td>[12774, 235]</td>
<td>[‘ç»’，’\x8d’]</td>
</tr>
<tr>
<td>词</td>
<td>[6744, 235]</td>
<td>[‘è¯’，’\x8d’]</td>
</tr>
<tr>
<td>好</td>
<td>[53901]</td>
<td>[‘好’]</td>
</tr>
</tbody></table>
<ul>
<li>可以看出除了能直接映射到中文token的文本，其他文本都是映射到unicode对应的token，因此token数量会比较多。同时因为一些中文被分成了unicode格式的字符，模型需要学习不同token组合起来是一个，因此模型对中文的理解不太好</li>
</ul>
<h3 id="常见模型词表大小"><a href="#常见模型词表大小" class="headerlink" title="常见模型词表大小"></a>常见模型词表大小</h3><table>
<thead>
<tr>
<th>模型</th>
<th>vocabulary size</th>
<th>tiktokenizer</th>
<th>上下文长度</th>
</tr>
</thead>
<tbody><tr>
<td>gpt2</td>
<td>50,257</td>
<td>gpt2</td>
<td>1k</td>
</tr>
<tr>
<td>gpt3</td>
<td>50,257</td>
<td>猜测与同样是gpt2</td>
<td>2k</td>
</tr>
<tr>
<td>gpt3.5</td>
<td>100,276</td>
<td>cl100k_base</td>
<td>4k  8k</td>
</tr>
<tr>
<td>gpt4</td>
<td>100,276</td>
<td>cl100k_base</td>
<td>8k  32k  128k</td>
</tr>
<tr>
<td>llama</td>
<td>32,000</td>
<td></td>
<td>2k</td>
</tr>
<tr>
<td>llama 2</td>
<td>32,000</td>
<td></td>
<td>4k</td>
</tr>
</tbody></table>
<details>
<summary>gpt分词器信息</summary>
<pre><code>
MODEL_TO_ENCODING: dict[str, str] = &#123;
    # chat
    "gpt-4": "cl100k_base",
    "gpt-3.5-turbo": "cl100k_base",
    "gpt-3.5": "cl100k_base",  # Common shorthand
    "gpt-35-turbo": "cl100k_base",  # Azure deployment name
    ......
    # open source
    "gpt2": "gpt2",
    "gpt-2": "gpt2",  # Maintains consistency with gpt-4
&#125;
</code></pre>
</details>


<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken/blob/main/tiktoken/model.py">来源</a>   对应的<a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py">vocabulary size</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://tiktokenizer.vercel.app/">在线的分词器网址</a></p>
</li>
<li><p>上下文长度 截止2024&#x2F;3&#x2F;21</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://platform.openai.com/docs/models/gpt-3-5-turbo">openai模型介绍</a></p>
<ul>
<li>模型名称里面带时间的是一个模型快照某个时间的快照，一般只维护三个月，且不再更新</li>
</ul>
</li>
</ul>
<h3 id="数据集预估"><a href="#数据集预估" class="headerlink" title="数据集预估"></a>数据集预估</h3><h4 id="分词测试"><a href="#分词测试" class="headerlink" title="分词测试"></a>分词测试</h4><table>
<thead>
<tr>
<th>分词器</th>
<th>长度</th>
<th>大小</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://nlp.cs.nyu.edu/wikipedia-data/data/articles.lst.gz">数据集</a></td>
<td>481,982,168</td>
<td>459 MB</td>
</tr>
<tr>
<td>gpt4</td>
<td>140,197,553</td>
<td>312M tokens \ GB</td>
</tr>
<tr>
<td>gpt3.5</td>
<td>140,197,553</td>
<td>312M tokens \ GB</td>
</tr>
<tr>
<td>gpt2</td>
<td>151,159,578</td>
<td>337M tokens \ GB</td>
</tr>
</tbody></table>
<ul>
<li><p>计算公式为长度 * 1024 &#x2F; 459</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Llfeng18/Ai_Exercise_Archive/blob/main/bpe/word_segmentation_example.py">分词示例</a></p>
</li>
<li><p>gpt2 40GB按上述测试比例估算大约13.5B tokens</p>
</li>
<li><p>按gpt3 570GB，300B tokens这个比例，gpt2 40GB大概21B，不知道为啥有这么大差异，可能数据集不同</p>
<ul>
<li>下文gpt2估算计算成本采用20B估算</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://platform.openai.com/docs/introduction">经验值</a> : 1个token 4个英文字母或者0.75个英文单词</p>
</li>
</ul>
<h3 id="内存和计算量增量"><a href="#内存和计算量增量" class="headerlink" title="内存和计算量增量"></a>内存和计算量增量</h3><h4 id="示例计算公式"><a href="#示例计算公式" class="headerlink" title="示例计算公式"></a>示例计算公式</h4><h5 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h5><p>以[nanogpt][<a target="_blank" rel="noopener" href="https://github.com/karpathy/nanoGPT/]%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%8C[%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B][https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb]">https://github.com/karpathy/nanoGPT/]计算的，[计算示例][https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb]</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">block_size = <span class="number">1024</span></span><br><span class="line">vocab_size = <span class="number">50257</span></span><br><span class="line">n_layer = <span class="number">12</span></span><br><span class="line">n_head = <span class="number">12</span></span><br><span class="line">n_embd = <span class="number">768</span></span><br><span class="line">bias = <span class="literal">False</span>     <span class="comment"># 不使用bias</span></span><br></pre></td></tr></table></figure>

<ul>
<li>nanogpt是复现gpt2的一个项目，实际gpt2是使用bias的</li>
</ul>
<h5 id="参数量"><a href="#参数量" class="headerlink" title="参数量"></a>参数量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">we see: 124337664, expected: 124337664, match: True</span><br><span class="line">name                 params     ratio (%) </span><br><span class="line">emebedding/position      786432     0.6325</span><br><span class="line">embedding/token        38597376    31.0424</span><br><span class="line">embedding              39383808    31.6749</span><br><span class="line">attention/ln                768     0.0006</span><br><span class="line">attention/kqv           1769472     1.4231</span><br><span class="line">attention/proj           589824     0.4744</span><br><span class="line">attention               2360064     1.8981</span><br><span class="line">mlp/ln                      768     0.0006</span><br><span class="line">mlp/ffw                 2359296     1.8975</span><br><span class="line">mlp/proj                2359296     1.8975</span><br><span class="line">mlp                     4719360     3.7956</span><br><span class="line">block                   7079424     5.6937</span><br><span class="line">transformer            84953088    68.3245</span><br><span class="line">ln_f                        768     0.0006</span><br><span class="line">dense                         0     0.0000</span><br><span class="line">total                 124337664   100.0000</span><br></pre></td></tr></table></figure>

<ul>
<li>实际使用内存 20(混合精度) * 124337664 &#x2F; 1e9 &#x3D; 2.48 GB</li>
<li>可以看出参数量主要集中在 embedding&#x2F;token （31.0424%）和 transformer（68.3245%）上<ul>
<li>embedding&#x2F;token即将输入文本映射到emb向量的编码表(这里编码和解码使用同一个)，计算公式为n_embd * vocab_size</li>
<li>emebedding&#x2F;position即将输入文本位置信息映射到emb向量，计算公式为n_embd * block_size</li>
<li>transformer主要是由12个block组成，这里不赘述</li>
</ul>
</li>
</ul>
<h5 id="计算量"><a href="#计算量" class="headerlink" title="计算量"></a>计算量</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">attention/kqv            3623878656     1.2426</span><br><span class="line">attention/scores         1610612736     0.5522</span><br><span class="line">attention/reduce         1610612736     0.5522</span><br><span class="line">attention/proj           1207959552     0.4142</span><br><span class="line">attention                8053063680     2.7612</span><br><span class="line">mlp/ffw1                 4831838208     1.6567</span><br><span class="line">mlp/ffw2                 4831838208     1.6567</span><br><span class="line">mlp                      9663676416     3.3135</span><br><span class="line">block                   17716740096     6.0747</span><br><span class="line">transformer            212600881152    72.8963</span><br><span class="line">dense                   79047426048    27.1037</span><br><span class="line">forward_total          291648307200   100.0000</span><br><span class="line">backward_total         583296614400   200.0000</span><br><span class="line">total                  874944921600   300.0000</span><br></pre></td></tr></table></figure>

<ul>
<li><p>这个是在不使用kv cache的情况下生成1个token的计算量，如果使用kv cache计算量应该是total + total &#x2F; block_size * max_new_tokens，max_new_tokens为预期生成的token数量</p>
</li>
<li><p>可以看出flops计算主要集中在 dense （27.1037%）和 transformer（72.8963%）上</p>
<ul>
<li><p>dense 即将输入文本和emb向量编码和解码，计算公式为2 * block_size * (n_embd * vocab_size)</p>
</li>
<li><p>transformer主要是由12个block组成，这里不赘述，计算公式为n_layer * (4 * block_size * block_size * n_embd + 24 * block_size * n_embd * n_embd)</p>
</li>
<li><p>block_size 包含在dense 和transformer的计算中</p>
</li>
</ul>
</li>
</ul>
<h4 id="不同模型分词和上下文长度增量计算"><a href="#不同模型分词和上下文长度增量计算" class="headerlink" title="不同模型分词和上下文长度增量计算"></a>不同模型分词和上下文长度增量计算</h4><p>本节旨在计算vocab size和block_size增加对于模型训练和推理过程中的内存和flops的增长关系</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>n_layer</th>
<th>n_embd</th>
<th>vocab -&gt; new vocab</th>
<th>上下文长度</th>
</tr>
</thead>
<tbody><tr>
<td>nanogpt</td>
<td>12</td>
<td>768</td>
<td>50,257 -&gt; 100,276 &#x3D; 0.768 &#x2F; 0.23 &#x2F; 0.078 &#x2F; 0.000076</td>
<td>1k -&gt; 8k &#x3D; 0.11 &#x2F;  0.033 &#x2F; 3.665 &#x2F; 0.00026</td>
</tr>
<tr>
<td>gpt3</td>
<td>96</td>
<td>12288</td>
<td>50,257 -&gt; 100,276 &#x3D; 12.29 &#x2F; 3.687 &#x2F; 2.51 &#x2F; 0.0012</td>
<td>2k -&gt; 8k &#x3D; 1.51 &#x2F;  0.452 &#x2F; 2,376.24 &#x2F; 0.029</td>
</tr>
<tr>
<td>gpt3.5</td>
<td>96(猜测)</td>
<td>12,288(猜测)</td>
<td>100,276 -&gt; 200,552 &#x3D; 24.64 &#x2F; 7.39 &#x2F; 10.09 &#x2F; 0.0024</td>
<td>4k -&gt; 32k &#x3D; 7.046 &#x2F;  2.11 &#x2F; 14,419.12 &#x2F; 0.135</td>
</tr>
<tr>
<td>gpt4</td>
<td>96(猜测)</td>
<td>15,447(猜测)</td>
<td>100,276 -&gt; 200,552 &#x3D; 30.979 &#x2F; 9.293 &#x2F; 25.38 &#x2F; 0.003</td>
<td>8k -&gt; 128k &#x3D; 37.96 &#x2F; 11.39 &#x2F; 160,164.4 &#x2F; 0.729</td>
</tr>
<tr>
<td>llama &#x2F; llama 2</td>
<td>80</td>
<td>8192</td>
<td>32000 -&gt; 64000 &#x3D; 5.24 &#x2F; 1.57 &#x2F; 1.073 &#x2F; 0.00052</td>
<td>2k -&gt; 8k &#x3D; 1.01 &#x2F; 0.301 &#x2F; 912.06 &#x2F; 0.024</td>
</tr>
</tbody></table>
<ul>
<li>flops计算：不使用kv cache的情况下生成1个token的计算量，训练也是这个计算量</li>
<li>增量flops计算：使用kv cache的情况下增量生成1个token的计算量，即上述公式flops计算 &#x2F; block_size </li>
<li>表格中的增量参数含义：训练内存计算(GB) &#x2F; 推理内存计算(GB) &#x2F; flops计算(TFLOPS) &#x2F; 增量flops计算(TFLOPS)</li>
<li>vocab_size 增量<ul>
<li>训练内存计算 : 20 * (new vocab - vocab) * n_embd &#x2F; 1e9 </li>
<li>推理内存计算 : 6 * (new vocab - vocab) * n_embd &#x2F; 1e9 </li>
<li>flops计算 : 2 * block_size * (n_embd * (new vocab - vocab)) &#x2F; 1e12</li>
<li>增量flops计算：2 *  (n_embd * (new vocab - vocab)) &#x2F; 1e12</li>
</ul>
</li>
<li>block_size 增量<ul>
<li>训练内存计算 : 20 * (new block_size - block_size ) * n_embd &#x2F; 1e9 </li>
<li>推理内存计算 : 6 * (new block_size - block_size) * n_embd &#x2F; 1e9 </li>
<li>flops计算 : (2 * (new block_size - block_size) * (n_embd * vocab) + n_layer * (4 * (new block_size - block_size) * (new block_size - block_size) * n_embd + 24 * (new block_size - block_size) * n_embd * n_embd)) &#x2F; 1e12</li>
<li>增量flops计算：( n_layer * (4 * (new block_size - block_size) * n_embd)) &#x2F; 1e12</li>
</ul>
</li>
<li>llama 模型架构与GPT有差异，这里按GPT的计算可能存在出入</li>
<li>gpt4 因为是moe架构，这里仅仅以一个头做计算可能存在出入</li>
<li>可以看出block_size的增加对flops的消耗更大</li>
</ul>
<h5 id="混合参数训练"><a href="#混合参数训练" class="headerlink" title="混合参数训练"></a>混合参数训练</h5><p>在一次训练迭代中，每个可训练模型参数都会对应1个梯度，并对应2个优化器状态（Adam优化器梯度的一阶动量和二阶动量）。设模型参数量为 Φ ，那么梯度的元素数量为 Φ ，AdamW优化器的元素数量为 2Φ 。float16数据类型的元素占2个bytes，float32数据类型的元素占4个bytes。在混合精度训练中，会使用float16的模型参数进行前向传递和后向传递，计算得到float16的梯度；在优化器更新模型参数时，会使用float32的优化器状态、float32的梯度、float32的模型参数来更新模型参数。</p>
<img src="/2024/04/04/Parameter-count/image-20240125111241738.png" alt="image-20240125111241738" style="zoom:50%;">

<ul>
<li><p>使用AdamW优化器和混合精度训练来训练参数量为 Φ 的大模型，<strong>模型参数、梯度和优化器状态占用的显存大小为</strong> 20Φ  。</p>
</li>
<li><p>如果是推理是，模型参数占用的显存大小为6Φ</p>
</li>
</ul>
<h2 id="模型参数量"><a href="#模型参数量" class="headerlink" title="模型参数量"></a>模型参数量</h2><h3 id="GPT系列"><a href="#GPT系列" class="headerlink" title="GPT系列"></a>GPT系列</h3><h4 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a><code>GPT-2</code></h4><table>
<thead>
<tr>
<th align="left">Model Name</th>
<th>n_params</th>
<th>n_layers</th>
<th>n_embd</th>
<th>n_heads</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GPT-2</td>
<td>117M</td>
<td>12</td>
<td>768</td>
<td>12</td>
</tr>
<tr>
<td align="left">GPT-2 Medium</td>
<td>345M</td>
<td>24</td>
<td>1024</td>
<td>16</td>
</tr>
<tr>
<td align="left">GPT-2 Large</td>
<td>762M</td>
<td>36</td>
<td>1280</td>
<td>20</td>
</tr>
<tr>
<td align="left">GPT-2 XL</td>
<td>1542B</td>
<td>48</td>
<td>1600</td>
<td>25</td>
</tr>
</tbody></table>
<ul>
<li>2019 年 2 月发布</li>
<li>训练数据量40GB文本，<a target="_blank" rel="noopener" href="https://github.com/karpathy/nanoGPT/tree/master/config">nanoGPT</a>使用300B tokens复现GPT2</li>
</ul>
<h4 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a><code>GPT-3</code></h4><table>
<thead>
<tr>
<th align="left">Model Name</th>
<th>n_params</th>
<th>n_layers</th>
<th>n_embd</th>
<th>n_heads</th>
<th>Batch Size</th>
<th>Learning Rate</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GPT-3 Small</td>
<td>125M</td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>0.5M</td>
<td>6.0 × 10^-4</td>
</tr>
<tr>
<td align="left">GPT-3 Medium</td>
<td>350M</td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>0.5M</td>
<td>3.0 × 10^-4</td>
</tr>
<tr>
<td align="left">GPT-3 Large</td>
<td>760M</td>
<td>24</td>
<td>1536</td>
<td>16</td>
<td>0.5M</td>
<td>2.5 × 10^-4</td>
</tr>
<tr>
<td align="left">GPT-3 XL</td>
<td>1.3B</td>
<td>24</td>
<td>2048</td>
<td>24</td>
<td>1M</td>
<td>2.0 × 10^-4</td>
</tr>
<tr>
<td align="left">GPT-3 2.7B</td>
<td>2.7B</td>
<td>32</td>
<td>2560</td>
<td>32</td>
<td>1M</td>
<td>1.6 × 10^-4</td>
</tr>
<tr>
<td align="left">GPT-3 6.7B</td>
<td>6.7B</td>
<td>32</td>
<td>4096</td>
<td>32</td>
<td>2M</td>
<td>1.2 × 10^-4</td>
</tr>
<tr>
<td align="left">GPT-3 13B</td>
<td>13.0B</td>
<td>40</td>
<td>5140</td>
<td>40</td>
<td>2M</td>
<td>1.0 × 10^-4</td>
</tr>
<tr>
<td align="left">GPT-3 175B or “GPT-3”</td>
<td>175.0B</td>
<td>96</td>
<td>12288</td>
<td>96</td>
<td>3.2M</td>
<td>0.6 × 10^-4</td>
</tr>
</tbody></table>
<ul>
<li>2020 年 5 月</li>
<li>训练数据量570GB文本，Trained on 300B tokens</li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ts4y1T7UH/?vd_source=bbacd2645e4ab14bd3e0a2dfb56a7a08">来源 State of GPT</a></li>
</ul>
<h4 id="GPT-3-5和GPT-4"><a href="#GPT-3-5和GPT-4" class="headerlink" title="GPT-3.5和GPT-4"></a><code>GPT-3.5和GPT-4</code></h4><ul>
<li><p>找不到准确的参数量这里以主流流传的数据计算</p>
</li>
<li><p>这里暂定GPT-3.5 与 GPT-3一致为175.0B(也有说法是20B)</p>
</li>
<li><p>GPT-4暂定8 * 220B，训练数据量为 13000B tokens。</p>
<ul>
<li>模型中还有约550亿个参数，被用做注意力机制的共享。</li>
<li>每次的前向传播推理（生成一个token）中，GPT-4只需要使用大约2800亿参数和560TFLOPs。相比之下，纯密集模型每次前向传播需要大约1.8 万亿个参数和约3700 TFLOP 的计算量。</li>
<li>与拥有1750亿参数的Davinchi模型相比，<strong>GPT-4的成本是其3倍</strong>，尽管其前馈参数只增加了1.6倍。这主要是因为GPT-4需要更大的集群，并且实现的利用率更低。</li>
<li>GPT-4的参数量大致比GPT-3.5(175.0B)大了1.25倍，用参数量$12lh^2$，大致估算是n_embd 15,447（存疑）</li>
</ul>
</li>
</ul>
<h3 id="LLaMA"><a href="#LLaMA" class="headerlink" title="LLaMA"></a>LLaMA</h3><h4 id="LLaMA-1"><a href="#LLaMA-1" class="headerlink" title="LLaMA"></a>LLaMA</h4><img src="/2024/04/04/Parameter-count/image-20240307171018254.png" alt="image-20240307171018254" style="zoom: 67%;">

<p>和 GPT 系列一样，LLaMA 模型也是 <code>Decoder-only</code> 架构，但结合前人的工作做了一些改进，比如：</p>
<ol>
<li>Pre-normalization [GPT3]. 为了提高训练稳定性，LLaMA 对每个 transformer 子层的输入进行归一化，使用 <code>RMSNorm</code> 归一化函数，Pre-normalization 由Zhang和Sennrich（2019）引入。</li>
<li><code>SwiGLU</code> 激活函数 [PaLM]. 将 ReLU 非线性替换为 <code>SwiGLU</code> 激活函数，且使用 $\frac{2}{3}4d$ 而不是 PaLM 论文中的 4d，SwiGLU 由 Shazeer（2020）引入以提高性能。</li>
<li><code>Rotary Embeddings</code> [GPTNeo]. 模型的输入不再使用 <code>positional embeddings</code>，而是在网络的每一层添加了 positional embeddings (RoPE)，RoPE 方法由Su等人（2021）引入。<ul>
<li>$ v’ &#x3D; \begin{pmatrix} \cos(\theta) &amp; -\sin(\theta) \ \sin(\theta) &amp; \cos(\theta) \end{pmatrix} \cdot v $​</li>
<li>$\theta &#x3D; p \times \Delta\theta$，其中位置 (p)， $\Delta\theta$ 是一个常数，代表每增加一个位置单位，旋转的角度增加多少。</li>
</ul>
</li>
</ol>
<ul>
<li><a target="_blank" rel="noopener" href="https://juejin.cn/post/7220985690795851836">来源</a></li>
</ul>
<h4 id="LLaMA-2"><a href="#LLaMA-2" class="headerlink" title="LLaMA 2"></a>LLaMA 2</h4><img src="/2024/04/04/Parameter-count/v2-7276937c93e0ab2cbb74ad33f3c83a8e_1440w.webp" alt="img" style="zoom:67%;">

<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/647862867">来源</a></li>
</ul>
<h2 id="算力需求"><a href="#算力需求" class="headerlink" title="算力需求"></a>算力需求</h2><h3 id="模型训练量"><a href="#模型训练量" class="headerlink" title="模型训练量"></a>模型训练量</h3><table>
<thead>
<tr>
<th align="left">Model Name</th>
<th>n_params</th>
<th>token</th>
<th>FLOPs</th>
<th>1 H100&#x2F;day</th>
<th>价格&#x2F;元</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GPT-2</td>
<td>0.117B</td>
<td>20B</td>
<td>14.04BB</td>
<td>0.55</td>
<td>236</td>
</tr>
<tr>
<td align="left">nanogpt</td>
<td>0.117B</td>
<td>300B</td>
<td>210.6BB</td>
<td>8.21</td>
<td>3532</td>
</tr>
<tr>
<td align="left">GPT-3 175B</td>
<td>175.0B</td>
<td>300B</td>
<td>315,000BB</td>
<td>12,287.94</td>
<td>528.7万</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td>280B(8x220B)</td>
<td>13000B</td>
<td>21,840,000BB</td>
<td>851,964.19</td>
<td>3.66亿</td>
</tr>
<tr>
<td align="left">LLaMA 2 7B</td>
<td>7B</td>
<td>2000B</td>
<td>84,000BB</td>
<td>3,276.78</td>
<td>140.9万</td>
</tr>
<tr>
<td align="left">LLaMA 2 13B</td>
<td>13B</td>
<td>2000B</td>
<td>156,000BB</td>
<td>6,085.45</td>
<td>261.8万</td>
</tr>
<tr>
<td align="left">LLaMA 2 34B</td>
<td>34B</td>
<td>2000B</td>
<td>408,000BB</td>
<td>15,915.81</td>
<td>684.8万</td>
</tr>
<tr>
<td align="left">LLaMA 2 70B</td>
<td>70B</td>
<td>2000B</td>
<td>840,000BB</td>
<td>32,767.85</td>
<td>1409.9万</td>
</tr>
</tbody></table>
<ul>
<li><p>LLaMA 的FLOPs是LLaMA 2的一半，这里不计算了</p>
</li>
<li><p>GPT-4 共有8x220B，有说到推理参数是280B，这里训练参数也以280B计算</p>
</li>
<li><p>nanogpt是复现GPT-2的一个项目 </p>
</li>
<li><p>GPT-3.5看着和GPT-3参数类似，这里不计算</p>
</li>
<li><p>FLOPs &#x3D; 6 *  n_params * token (不使用重计算)</p>
</li>
<li><p>BB是B * B的含义</p>
</li>
<li><p>H100 989 TFLOPS（Tensor Float16） 80GB 价格2.49美元&#x2F;17.928人民币&#x2F;卡&#x2F;h(2024&#x2F;2&#x2F;23 <a target="_blank" rel="noopener" href="https://lambdalabs.com/">https://lambdalabs.com/</a>)</p>
</li>
<li><p>1 H100&#x2F;day &#x3D; FLOPs &#x2F; 989,000,000,000,000 &#x2F; 0.3 &#x2F; 3600 &#x2F; 24 </p>
<ul>
<li>0.3为gpu利用率</li>
</ul>
</li>
<li><p>1 H100&#x2F;day 元 430.272元&#x2F;day</p>
</li>
</ul>
<h3 id="推理成本"><a href="#推理成本" class="headerlink" title="推理成本"></a>推理成本</h3><table>
<thead>
<tr>
<th align="left">Model Name</th>
<th>block_size</th>
<th>vocab_size</th>
<th>n_layer</th>
<th>n_head</th>
<th>n_embd</th>
<th>flops_total</th>
<th>flops_per</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GPT-2</td>
<td>1k</td>
<td>50,257</td>
<td>12</td>
<td>12</td>
<td>768</td>
<td>0.291648 \ 0.000005</td>
<td>0.000285 \ 0.00000000</td>
</tr>
<tr>
<td align="left">GPT-3 175B</td>
<td>2k</td>
<td>50,257</td>
<td>96</td>
<td>96</td>
<td>12288</td>
<td>734.8042 \ 0.012333</td>
<td>0.358791 \ 0.00000602</td>
</tr>
<tr>
<td align="left">GPT-3.5</td>
<td>4k</td>
<td>100,276</td>
<td>96(猜测)</td>
<td>96(猜测)</td>
<td>12288(猜测)</td>
<td>1514.226 \ 0.025416</td>
<td>0.369684 \ 0.00000621</td>
</tr>
<tr>
<td align="left">GPT-3.5</td>
<td>8k</td>
<td>100,276</td>
<td>96(猜测)</td>
<td>96(猜测)</td>
<td>12288(猜测)</td>
<td>3186.781 \ 0.053489</td>
<td>0.389011 \ 0.00000653</td>
</tr>
<tr>
<td align="left">GPT-3.5</td>
<td>16k</td>
<td>100,276</td>
<td>96(猜测)</td>
<td>96(猜测)</td>
<td>12288(猜测)</td>
<td>7006.882 \ 0.117608</td>
<td>0.427666 \ 0.00000718</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td>8k</td>
<td>100,276</td>
<td>96(猜测)</td>
<td>96(猜测)</td>
<td>15,447(猜测)</td>
<td>4925.932 \ 0.082680</td>
<td>0.601310 \ 0.00001009</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td>32k</td>
<td>100,276</td>
<td>96(猜测)</td>
<td>96(猜测)</td>
<td>15,447(猜测)</td>
<td>24467.07 \ 0.410671</td>
<td>0.746676 \ 0.00001253</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td>128k</td>
<td>100,276</td>
<td>96(猜测)</td>
<td>96(猜测)</td>
<td>15,447(猜测)</td>
<td>174081.7 \ 2.921898</td>
<td>1.328138 \ 0.00002229</td>
</tr>
</tbody></table>
<ul>
<li>flops_total计算：不使用kv cache的情况下生成1个token的计算量，训练也是这个计算量</li>
<li>flops_per计算：使用kv cache的情况下增量生成1个token的计算量，即上述公式flops计算 &#x2F; block_size </li>
<li>表格中的参数含义：flops计算(TFLOPS) &#x2F; 算力成本(元)</li>
<li>算力成本使用上面的H100估算，FLOPs &#x2F; 989,000,000,000,000 &#x2F; 0.3 &#x2F; 3600 &#x2F; 24 * 430.272，<a target="_blank" rel="noopener" href="https://github.com/Llfeng18/Ai_Exercise_Archive/blob/main/bpe/flops_inference.py">计算示例</a></li>
</ul>
<table>
<thead>
<tr>
<th align="left">Model Name</th>
<th>block_size</th>
<th>Input</th>
<th>Output</th>
<th>flops_total \ flops_per</th>
<th>Input \ Output  除flops_per</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GPT-3.5</td>
<td>4k</td>
<td>$1.50 &#x2F; 1M tokens     0.0000108 &#x2F; token</td>
<td>$2.00 &#x2F; 1M tokens       0.0000144  &#x2F; token</td>
<td>0.025416 \ 0.00000621</td>
<td>173.9% \ 231.8%</td>
</tr>
<tr>
<td align="left">GPT-3.5</td>
<td>16k</td>
<td>$0.50 &#x2F; 1M tokens     0.0000036 &#x2F; token</td>
<td>$1.50 &#x2F; 1M tokens       0.0000108  &#x2F; token</td>
<td>0.117608 \ 0.00000718</td>
<td>50.1% \ 150.4%</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td>8k</td>
<td>$30.00 &#x2F; 1M tokens   0.000216 &#x2F; token</td>
<td>$60.00 &#x2F; 1M tokens     0.000432 &#x2F; token</td>
<td>0.082680 \ 0.00001009</td>
<td>2140.7% \ 4281.4%</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td>32k</td>
<td>$60.00 &#x2F; 1M tokens   0.000432 &#x2F; token</td>
<td>$120.00 &#x2F; 1M tokens   0.000864 &#x2F; token</td>
<td>0.410671 \ 0.00001253</td>
<td>3447.7% \ 6895.4%</td>
</tr>
<tr>
<td align="left">GPT-4 Turbo</td>
<td>128k</td>
<td>$10.00 &#x2F; 1M tokens   0.000072 &#x2F; token</td>
<td>$30.00 &#x2F; 1M tokens     0.000216 &#x2F; token</td>
<td>2.921898 \ 0.00002229</td>
<td>323.0% \ 969.0%</td>
</tr>
</tbody></table>
<ul>
<li><p>美元兑人民币使用7.2汇率,</p>
</li>
<li><p>表格中的参数含义：美元 &#x2F; M tokens &#x2F; 人民币 &#x2F; token</p>
</li>
<li><p>flops_total \ flops_per为上个表格计算的推理成本</p>
</li>
<li><p>从表格看出估算方式有一些问题主要在于</p>
<ol>
<li>gpt官方output价格大于input价格的，这里我用的是一样的进行计算的，我理解的是input价格应该是跟token数量成正比的，那么output如果是使用kv cache情况下，理论上每次只需要计算一个新生成的token得到下一个token，更贵的原因是因为kv cache占用内存太高，因此部分使用了重计算？</li>
<li>gpt模型提升上下文长度反而降低了售价，与我计算的结论相反，这是因为模型计算效率提升，还是对输出做了一些特殊处理？</li>
<li>gpt4模型用的是moe架构，这里仅仅以一个头做计算可能存在出入</li>
</ol>
</li>
</ul>
<h2 id="显存占用"><a href="#显存占用" class="headerlink" title="显存占用"></a>显存占用</h2><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="参数占用显存"><a href="#参数占用显存" class="headerlink" title="参数占用显存"></a>参数占用显存</h4><p>假设使用AdamW优化器和混合精度训练来训练参数量为 Φ 的大模型，<strong>模型参数、梯度和优化器状态占用的显存大小为</strong> 20Φ  。</p>
<img src="/2024/04/04/Parameter-count/image-20240310093550323.png" alt="image-20240310093550323" style="zoom:67%;">



<table>
<thead>
<tr>
<th align="left">Model Name</th>
<th>n_params</th>
<th>Training_memory</th>
<th>Inference_memory</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GPT-2</td>
<td>0.117B</td>
<td>2.179GB</td>
<td>0.2179GB</td>
</tr>
<tr>
<td align="left">GPT-3 175B</td>
<td>175.0B</td>
<td>3259GB</td>
<td>325.9GB</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td>280B(8x220B)</td>
<td>32782GB</td>
<td>3278.2GB</td>
</tr>
<tr>
<td align="left">LLaMA 2 7B</td>
<td>7B</td>
<td>130GB</td>
<td>13.0GB</td>
</tr>
<tr>
<td align="left">LLaMA 2 13B</td>
<td>13B</td>
<td>242GB</td>
<td>24.2GB</td>
</tr>
<tr>
<td align="left">LLaMA 2 34B</td>
<td>34B</td>
<td>633GB</td>
<td>63.3GB</td>
</tr>
<tr>
<td align="left">LLaMA 2 70B</td>
<td>70B</td>
<td>1303GB</td>
<td>130.3GB</td>
</tr>
</tbody></table>
<h4 id="激活值"><a href="#激活值" class="headerlink" title="激活值"></a>激活值</h4><p>这里的激活（activations）指的是：<strong>前向传递过程中计算得到的，并在后向传递过程中需要用到的所有张量</strong>。</p>
<p>对于 l 层transformer模型，中间激活占用的显存大小可以近似为$l(34bsh + 5bs^2a)$</p>
<p>以GPT3-175B为例，我们来直观地对比下模型参数与中间激活的显存大小。GPT3的模型配置如下。我们假设采用混合精度训练，模型参数和中间激活都采用float16数据类型，每个元素占2个bytes。</p>
<img src="/2024/04/04/Parameter-count/image-20240125113050750.png" alt="image-20240125113050750" style="zoom:50%;">



<p>GPT3的模型参数量为175B，占用的显存大小为 $2 \times 175 \times 10^9 \text{ bytes} &#x3D; 350GB $ 。GPT3模型需要占用350GB的显存。</p>
<p>GPT3的序列长度 s 为 2048 。对比不同的批次大小 b占用的中间激活：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">96 * (34 * 2048 * 12288 + 5 * 2048^2 * 96) * 1 / 1e9 = 275.41</span><br><span class="line">96 * (34 * 2048 * 12288 + 5 * 2048^2 * 96) * 64 / 1e9 = 17,626.5</span><br><span class="line">96 * (34 * 2048 * 12288 + 5 * 2048^2 * 96) * 128 / 1e9 = 35,253</span><br></pre></td></tr></table></figure>

<p>当 b&#x3D;1 时，中间激活占用显存为275GB，大约是模型参数显存的0.79倍。</p>
<p>当 b&#x3D;64 时，中间激活占用显存为 17.6TB ，大约是模型参数显存的50倍。</p>
<p>当 b&#x3D;128 时，中间激活占用显存为 35.3TB ，大约是模型参数显存的101倍。</p>
<p>可以看到随着批次大小 b 的增大，中间激活占用的显存远远超过了模型参数显存。通常会采用<strong>激活重计算</strong>技术来减少中间激活，理论上可以将中间激活显存从 O(n) 减少到 $O(\sqrt(n)) $，代价是增加了一次额外前向计算的时间，本质上是“时间换空间”。</p>
<h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p>如果使用float16来进行推理，<strong>推理阶段模型参数占用的显存大概是</strong> 2Φ  。</p>
<h4 id="KV-cache"><a href="#KV-cache" class="headerlink" title="KV cache"></a>KV cache</h4><p>假设输入序列的长度为 s ，输出序列的长度为 n ，以float16来保存KV cache，那么<strong>KV cache的峰值显存占用大小为</strong> $b(s + n)hl \times 2 \times 2 &#x3D; 4bhl(s + n) $。这里第一个2表示K&#x2F;V cache，第二个2表示float16占2个bytes。</p>
<p>以GPT3为例，对比KV cache与模型参数占用显存的大小。GPT3模型占用显存大小为350GB。假设批次大小 b&#x3D;64 ，输入序列长度 s&#x3D;512 ，输出序列长度 n&#x3D;32 ，则KV cache占用显存为 $ 4bhl(s + n) &#x3D; 164,282,499,072 \text{ bytes} \approx 164GB $ ，大约是模型参数显存的0.5倍。</p>
<h2 id="显卡"><a href="#显卡" class="headerlink" title="显卡"></a>显卡</h2><img src="/2024/04/04/Parameter-count/MBXY-CR-deab4f3ddb8e4e128bec48d43632f5e7.png" alt="img" style="zoom:67%;">

<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.eet-china.com/mp/a301982.html">来源</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://moonshot.feishu.cn/sheets/OaBLsKXazhMXjLt9dw5cC4Gvnzc">国外显卡算力图</a></p>
</li>
</ul>
<h2 id="api"><a href="#api" class="headerlink" title="api"></a>api</h2><p><a target="_blank" rel="noopener" href="https://openai.com/pricing">openai</a></p>
<p><a target="_blank" rel="noopener" href="https://www.anthropic.com/api#pricing">claude</a></p>
<p><a target="_blank" rel="noopener" href="https://leaderboard.withmartian.com/">https://leaderboard.withmartian.com/</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/683636677">https://zhuanlan.zhihu.com/p/683636677</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/683359705">https://zhuanlan.zhihu.com/p/683359705</a></p>
<h2 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h2><ul>
<li>本文的计算只使用了单一显卡计算成本，后续计算其他线卡</li>
<li>本文未估计显卡互联的最大张数，能组合成的集群，后续看一下怎么计算</li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Llfeng
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2024/04/04/Parameter-count/" title="Parameter count">http://example.com/2024/04/04/Parameter-count/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/11/26/DeepLearning/" rel="prev" title="DeepLearning">
      <i class="fa fa-chevron-left"></i> DeepLearning
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81OTEwMi8zNTU2NA=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E8%AF%8D%E5%92%8C%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6"><span class="nav-number">1.</span> <span class="nav-text">分词和上下文长度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D"><span class="nav-number">1.1.</span> <span class="nav-text">中文分词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B%E8%AF%8D%E8%A1%A8%E5%A4%A7%E5%B0%8F"><span class="nav-number">1.2.</span> <span class="nav-text">常见模型词表大小</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E9%A2%84%E4%BC%B0"><span class="nav-number">1.3.</span> <span class="nav-text">数据集预估</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E8%AF%8D%E6%B5%8B%E8%AF%95"><span class="nav-number">1.3.1.</span> <span class="nav-text">分词测试</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%92%8C%E8%AE%A1%E7%AE%97%E9%87%8F%E5%A2%9E%E9%87%8F"><span class="nav-number">1.4.</span> <span class="nav-text">内存和计算量增量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="nav-number">1.4.1.</span> <span class="nav-text">示例计算公式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">模型参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E9%87%8F"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">参数量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E9%87%8F"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">计算量</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E6%A8%A1%E5%9E%8B%E5%88%86%E8%AF%8D%E5%92%8C%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6%E5%A2%9E%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="nav-number">1.4.2.</span> <span class="nav-text">不同模型分词和上下文长度增量计算</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E5%8F%82%E6%95%B0%E8%AE%AD%E7%BB%83"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">混合参数训练</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%87%8F"><span class="nav-number">2.</span> <span class="nav-text">模型参数量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GPT%E7%B3%BB%E5%88%97"><span class="nav-number">2.1.</span> <span class="nav-text">GPT系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT-2"><span class="nav-number">2.1.1.</span> <span class="nav-text">GPT-2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT-3"><span class="nav-number">2.1.2.</span> <span class="nav-text">GPT-3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT-3-5%E5%92%8CGPT-4"><span class="nav-number">2.1.3.</span> <span class="nav-text">GPT-3.5和GPT-4</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLaMA"><span class="nav-number">2.2.</span> <span class="nav-text">LLaMA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaMA-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">LLaMA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaMA-2"><span class="nav-number">2.2.2.</span> <span class="nav-text">LLaMA 2</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E5%8A%9B%E9%9C%80%E6%B1%82"><span class="nav-number">3.</span> <span class="nav-text">算力需求</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%87%8F"><span class="nav-number">3.1.</span> <span class="nav-text">模型训练量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E6%88%90%E6%9C%AC"><span class="nav-number">3.2.</span> <span class="nav-text">推理成本</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8"><span class="nav-number">4.</span> <span class="nav-text">显存占用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">4.1.</span> <span class="nav-text">训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%8D%A0%E7%94%A8%E6%98%BE%E5%AD%98"><span class="nav-number">4.1.1.</span> <span class="nav-text">参数占用显存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%80%BC"><span class="nav-number">4.1.2.</span> <span class="nav-text">激活值</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86"><span class="nav-number">4.2.</span> <span class="nav-text">推理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#KV-cache"><span class="nav-number">4.2.1.</span> <span class="nav-text">KV cache</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%98%BE%E5%8D%A1"><span class="nav-number">5.</span> <span class="nav-text">显卡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#api"><span class="nav-number">6.</span> <span class="nav-text">api</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#todo"><span class="nav-number">7.</span> <span class="nav-text">todo</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Llfeng"
      src="/images/img.png">
  <p class="site-author-name" itemprop="name">Llfeng</p>
  <div class="site-description" itemprop="description">Welcome</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Llfeng18" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Llfeng18" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1481314343@qq.com" title="E-Mail → mailto:1481314343@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2023-10 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Llfeng</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">14k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">13 分钟</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v7.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.2
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
